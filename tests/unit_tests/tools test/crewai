from crewai_tools import ScrapeWebsiteTool
import os
import json
from langchain_community.document_loaders import TextLoader
from os.path import join
import logging
import urllib3
from langchain.document_loaders import GithubFileLoader

urllib3.disable_warnings()

# Create a logger for the 'azure' SDK
logger = logging.getLogger('azure')
logger.setLevel(logging.ERROR)

# take the local settgins file and convert it into environemnt variables
settings = json.loads(TextLoader(join(os.getcwd(), 'local.settings.json'), encoding="utf-8").load()[0].page_content)

for setting in settings["Values"]:
    os.environ[setting]=settings["Values"][setting]

# # To enable scrapping any website it finds during it's execution
# tool = ScrapeWebsiteTool()

# # Initialize the tool with the website URL, so the agent can only scrap the content of the specified website
# tool = ScrapeWebsiteTool(website_url='https://www.bbc.co.uk')

# # Extract the text from the site
# text = tool.run()
# print(text)


from crewai_tools import GithubSearchTool
token=os.getenv("GITHUB_KEY")

loader = GithubFileLoader(
    repo="ukho/docs",  # the repo name
    access_token=token,
    github_api_url="https://api.github.com",
    file_filter=lambda file_path: file_path.endswith(
        ".md"
    ),  # load all markdowns files.
)
documents = loader.load()
print(documents)